%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2016 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2016,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
% \usepackage{subfigure} % should use either subfigure (older) or subfig
% (newer). subcaption (not used at all) is most current and might be a choice
% to consider in the future.

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
%\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[nohyperref]{icml2016}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}

\input{packages.tex}
\usepackage{subfig}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learning Representational Brain Networks}

\begin{document}

\twocolumn[
\icmltitle{Learning Representational Brain Networks from fMRI}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2016
% package.
\icmlauthor{Urvashi Oswal}{uoswal@wisc.edu}
\icmladdress{Department of Electrical and Computer Engineering, University of Wisconsin-Madison, WI, 53706 USA}

\icmlauthor{Christopher Cox}{crcox@wisc.edu}
\icmladdress{Department of Psychology, University of Wisconsin-Madison, WI, 53706 USA}

\icmlauthor{Matthew A. Lambon Ralph}{matt.lambon-ralph@manchester.ac.uk}
\icmladdress{Neuroscience and Aphasia Research Unit (NARU), School of Psychological Sciences, University of Manchester, Manchester M13 9PL, UK}

\icmlauthor{Timothy T. Rogers}{ttrogers@wisc.edu}
\icmladdress{Department of Psychology, University of Wisconsin-Madison, WI, 53706 USA}

\icmlauthor{Robert Nowak}{nowak@ece.wisc.edu}
\icmladdress{Department of Electrical and Computer Engineering, University of Wisconsin-Madison, WI, 53706 USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
\input{abstract.tex}
\end{abstract} 

	
	\section{Introduction}
	\input{intro.tex}
	
	\section{GrOWL}
	\label{Sec:growl}
	\input{growl.tex}
	
	\section{Network RSA: Simulated Data}
        \label{wbrsa1}
	\input{simulation.tex}
	
	\section{Network RSA: Real Data}
        \label{wbrsa2}
	\input{application.tex}
	
	\section{Conclusion}
We have developed and demonstrated a new approach for whole-brain Representational Similarity Analysis called Network RSA (NRSA). Unlike traditional RSA methods that consider only specific regions of interest or spherical clusters of the cortex,  NRSA can discover arbitrarily structured brain networks (possibly widely distributed and non-local) that encode similarity information.  NRSA is posed as a sparsity-regularized multi-task regression problem. This allows us to effectively search over all subsets of voxels (not just localized clusters) to detect similarity-encoding networks.  We proposed a new sparsity regularizer for multi-task regression that is able to cope with strongly correlated covariates (voxels in the fMRI application),
which can perform better than the conventional group lasso. named the GrOWL.  Experiments with real and synthetic datasets demonstrated the potential of our new approach.
	
	\section{Clustering properties of GrOWL with absolute error loss function}
	\input{supplement1.tex}
	
	\section{Clustering properties of GrOWL with squared Frobenius loss function}
	\input{supplement.tex}
	
	\section{Proximal algorithms for GrOWL}
	\input{supplement2.tex}
	% you can choose not to have a title for an appendix
	% if you want by leaving the argument blank
	

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

%\bibliography{example_paper}
	\begin{thebibliography}{20}
		
		%\bibitem{IEEEhowto:kopka}
		%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
	%	0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
		
		\bibitem{RSA} N.~Kriegeskorte, M.~Mur, and P.~Bandettini, ``Representational similarity analysis--connecting the branches of systems neuroscience", \emph{Frontiers in systems neuroscience}, 2, 2008.
			\bibitem{searchlight} N.~Kriegeskorte, R.~Goebel, and P.~Bandettini,  ``Information-based functional brain mapping", \emph{Proceedings of the National Academy of Sciences of the United States of America, 103} vol.~10, pp.~3863--3868, 2006.
		
	
		\bibitem{multitask1} A.~Argyriou, T.~Evgeniou, and M.~Pontil. ``Convex multi-task feature learning",  \emph{Machine Learning, 3},
vol.~73,, pp.~243--272, 2008.
		
		\bibitem{similarity}A.~Tversky, and I.~Gati, ``Similarity, separability, and the triangle inequality", \emph{Psychological review, 2} vol.~89, pp.~123, 1982.
		
		\bibitem{obo11}
		G.~Obozinski, M.~Wainwright and M.~Jordan, ``Support union recovery in high-dimensional multivariate regression," {\it Ann. Stat.}, pp.~1--47, 2011.
		
		\bibitem{oscar}
		H.~Bondell and B.~Reich, ``Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with OSCAR," {\it Biometrics}, vol.~64, pp.~115--123, 2007.
		
		
		\bibitem{Dalton}
		H.~Dalton, ``The measurement of the inequality of incomes," {\it The Economic Journal}, vol.~30, pp.~348--361, 1920.
		
		
		%\bibitem{smart}H.~Wang, F.~Nie, H.~Huang, S.~Risacher, C.~Ding, A.~Saykin, and L.~Shen, ``Sparse multi-task regression and feature selection to identify brain imaging predictors for memory performance", \emph{IEEE International Conference on Computer Vision (ICCV)}, pp.~557--562, 2011.
		
		
		\bibitem{EN}H.~Zou, and T.~Hastie, ``Regularization and variable selection via the elastic net. \emph{Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67} vol.~2, pp.~301--320, 2005
		
		\bibitem{lounici} K.~Lounici, M.~Pontil, A.~Tsybakov, and S.~van de Geer. ``Taking advantage of sparsity in multi-task learning." \url{http://arxiv.org/abs/0903.1468}, 2009.
		
		\bibitem{vandegeer} K.~Lounici, M.~Pontil, S.~van de Geer, and A.~Tsybakov, ``Oracle inequalities and optimal inference under group sparsity", \emph{The Annals of Statistics, 39} vol.~4, pp.~2164--2204, 2012.
		
		
		\bibitem{candes13}
		M.~Bogdan, E.~van den Berg, W.~Su, and E.~Cand\`{e}s, ``Statistical estimation and testing via the sorted $\ell_1$ norm,''  available at \url{http://arxiv.org/abs/1310.1969}, 2013.
		
		\bibitem{bogdan2014}
		M.~Bogdan, E.~van~den~Berg, C.~Sabatti, W.~Su, and E.~Cand\`{e}s, ``SLOPE -- adaptive variable selection via convex optimization", available at \url{http://arxiv.org/abs/1407.3824}, 2014.
		
		
		\bibitem{owl} M.~Figueiredo, and R.~Nowak, ``Sparse Estimation with Strongly Correlated Variables using Ordered Weighted l1 Regularization", \url{http://arxiv.org/abs/1409.4005}, 2014.
		
		\bibitem{prox} N.~Parikh, and S.~Boyd, ``Proximal algorithms.", {\em Foundations and Trends in optimization 3} vol.~1, pp.~123--231, 2013.
		
		\bibitem{buhlmann13}
		P.~B\"{u}hlmann, P.~R\"{u}ttiman, S.~van~de~Geer, and C.-H.~Zhang, ``Correlated variables in regression: Clustering and sparse estimation,'' {\em Journal of Statistical Planning and Inference}, pp.~1835--1858, 2013.
		
		%\bibitem{prox}
		%P.~Combettes, \fix D.~D{\~u}ng, and B.~V{\~u}, "Proximity for sums of composite functions",\emph{Journal of Mathematical Analysis and applications, 380} vol.~2, pp.~680--688, 2011.
		
		\bibitem{maryam} S.~Oymak, A.~Jalali, M.~Fazel, Y.~Eldar, and B.~Hassibi, ``Simultaneously structured models with application to sparse and low-rank matrices", \url{http://arxiv.org/abs/1212.3753}, 2012.
		
		\bibitem{ZengFigueiredo2013}
		X.~Zeng, M.~Figueiredo, ``Decreasing Weighted Sorted $\ell_1$ Regularization", {\em IEEE Signal Processing Letters,} vol.~21, pp.~1240--1244, 2014.
		
		\bibitem{ZengFigueiredo2014}
		X.~Zeng, M.~Figueiredo, ``The atomic norm formulation of OSCAR regularization with application to the Frank-Wolfe algorithm", {\em Proceedings of the European Signal Processing Conference}, Lisbon, Portugal, 2014.
		
		
		
	\end{thebibliography}
\bibliographystyle{icml2016}
	
\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
